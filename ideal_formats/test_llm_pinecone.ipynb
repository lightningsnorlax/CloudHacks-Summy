{
 "cells": [
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Experiments with promopts\n",
    "\n",
    "Assumed Pinecone db is already created and populated with data."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 1,
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "c:\\Users\\Soh Hong Yu\\anaconda3\\lib\\site-packages\\pinecone\\index.py:4: TqdmExperimentalWarning: Using `tqdm.autonotebook.tqdm` in notebook mode. Use `tqdm.tqdm` instead to force console mode (e.g. in jupyter console)\n",
      "  from tqdm.autonotebook import tqdm\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "PATH_TO_ENV:  c:\\Users\\Soh Hong Yu\\OneDrive\\Documents\\GitHub\\CloudHacks-Summy\\ideal_formats\\.env\n"
     ]
    }
   ],
   "source": [
    "import pinecone\n",
    "import openai\n",
    "import numpy as np\n",
    "import os\n",
    "from dotenv import load_dotenv\n",
    "\n",
    "# Langchain\n",
    "from langchain.embeddings.openai import OpenAIEmbeddings\n",
    "from langchain.text_splitter import CharacterTextSplitter\n",
    "from langchain.vectorstores import Pinecone\n",
    "from langchain.document_loaders import TextLoader\n",
    "from langchain.chat_models import ChatOpenAI\n",
    "from langchain.chains import RetrievalQA, LLMChain ,LLMCheckerChain\n",
    "from langchain.callbacks import wandb_tracing_enabled\n",
    "from langchain.prompts import (\n",
    "    PromptTemplate,\n",
    "    ChatPromptTemplate,\n",
    "    HumanMessagePromptTemplate,\n",
    ")\n",
    "from langchain.prompts.few_shot import FewShotPromptTemplate\n",
    "\n",
    "from typing import Optional\n",
    "from langchain.chains import SimpleSequentialChain ,SequentialChain\n",
    "\n",
    "from langchain.chains.openai_functions import (\n",
    "    create_openai_fn_chain,\n",
    "    create_structured_output_chain,\n",
    ")\n",
    "from langchain.schema import HumanMessage, AIMessage, ChatMessage\n",
    "\n",
    "# wandb\n",
    "import wandb \n",
    "\n",
    "# Import singlife from utils \n",
    "from utils.singlife import Singlife"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Initialization\n",
    "\n",
    "* Load all the API keys from Pinecone and OpenAI\n",
    "* Load the Pinecone client\n",
    "* Set-up wandb tracing"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Load variables from the .env file\n",
    "load_dotenv('../Sn33k/.env')\n",
    "\n",
    "# Access the variables\n",
    "OPENAI_API_KEY = os.environ.get(\"OPENAI_API_KEY\")\n",
    "PINECONE_API_KEY = os.environ.get(\"PINECONE_API_KEY\")\n",
    "INDEX_NAME = os.environ.get(\"PINECONE_INDEX_NAME\")\n",
    "PINECONE_ENVIRONMENT= os.environ.get(\"PINECONE_ENVIRONMENT\")\n",
    "\n",
    "openai.api_key = OPENAI_API_KEY\n",
    "os.environ[\"OPENAI_API_KEY\"] = OPENAI_API_KEY"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## WANDB setup"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## LLM Setup"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "{'lc': 1,\n",
       " 'type': 'constructor',\n",
       " 'id': ['langchain', 'chat_models', 'openai', 'ChatOpenAI'],\n",
       " 'kwargs': {'model_name': 'gpt-4-0613',\n",
       "  'temperature': 0.0,\n",
       "  'openai_api_key': {'lc': 1, 'type': 'secret', 'id': ['OPENAI_API_KEY']}}}"
      ]
     },
     "execution_count": 3,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "model_name = \"gpt-4-0613\"\n",
    "temperature = 0.0\n",
    "llm_qa = ChatOpenAI(model_name=model_name, temperature=temperature)\n",
    "llm_qa.to_json()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "{'lc': 1,\n",
       " 'type': 'constructor',\n",
       " 'id': ['langchain', 'chat_models', 'openai', 'ChatOpenAI'],\n",
       " 'kwargs': {'model_name': 'gpt-3.5-turbo-0613',\n",
       "  'temperature': 0.0,\n",
       "  'model_kwargs': {'functions': [{'name': 'output_formatter',\n",
       "     'description': 'Output formatter. Should always be used to format your response to the user.',\n",
       "     'parameters': {'title': 'generate_video_script',\n",
       "      'description': 'Generates 15-30sec video script based on custom knowledge base. Two components 1.Scene assets descriptions 2.Subtitle script',\n",
       "      'type': 'object',\n",
       "      'properties': {'list_of_video_chunk': {'type': 'array',\n",
       "        'description': 'List of video_chunk to be included in the video, one video chunk should last 3-5 seconds and includes: 1. Scene  2. Subtitle',\n",
       "        'items': {'type': 'object'},\n",
       "        'properties': {'scene': {'type': 'string',\n",
       "          'description': 'Scene description for video should be visual and general'},\n",
       "         'subtitles': {'type': 'string',\n",
       "          'description': 'Funny and sarcastic video subtitles script for video'}},\n",
       "        'required': ['scene', 'subtitles']}}}}],\n",
       "   'function_call': {'name': 'output_formatter'}},\n",
       "  'openai_api_key': {'lc': 1, 'type': 'secret', 'id': ['OPENAI_API_KEY']}}}"
      ]
     },
     "execution_count": 4,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "model_name = \"gpt-3.5-turbo-0613\"\n",
    "temperature = 0.0\n",
    "llm_video_script = ChatOpenAI(model_name=model_name, temperature=temperature, model_kwargs= {\"functions\":[\n",
    "    {\n",
    "      \"name\": \"output_formatter\",\n",
    "      \"description\": \"Output formatter. Should always be used to format your response to the user.\",\n",
    "      \"parameters\": {\n",
    "        \"title\": \"generate_video_script\",\n",
    "        \"description\": \"Generates 15-30sec video script based on custom knowledge base. Two components 1.Scene assets descriptions 2.Subtitle script\",\n",
    "        \"type\": \"object\",\n",
    "        \"properties\": {\n",
    "          \"list_of_video_chunk\": {\n",
    "            \"type\": \"array\",\n",
    "            \"description\": \"List of video_chunk to be included in the video, one video chunk should last 3-5 seconds and includes: 1. Scene  2. Subtitle\",\n",
    "            \"items\": {\n",
    "              \"type\": \"object\"\n",
    "            },\n",
    "            \"properties\": {\n",
    "              \"scene\": {\n",
    "                \"type\": \"string\",\n",
    "                \"description\": \"Scene description for video should be visual and general\"\n",
    "              },\n",
    "              \"subtitles\": {\n",
    "                \"type\": \"string\",\n",
    "                \"description\": \"Funny and sarcastic video subtitles script for video\"\n",
    "              }\n",
    "            },\n",
    "            \"required\": [\"scene\", \"subtitles\"]\n",
    "          }\n",
    "        },\n",
    "      }\n",
    "    }\n",
    "],\"function_call\":{\"name\":\"output_formatter\"}})\n",
    "llm_video_script.to_json()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Vectorstore Setup ( Pinecone )\n",
    "\n",
    "Pinecone integration with Langchain"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Initialize Pinecone client"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "index_description:  IndexDescription(name='cloudhacks-summy', metric='cosine', replicas=1, dimension=1536.0, shards=1, pods=1, pod_type='starter', status={'ready': True, 'state': 'Ready'}, metadata_config=None, source_collection='')\n",
      "index_stats_response:  {'dimension': 1536,\n",
      " 'index_fullness': 0.00115,\n",
      " 'namespaces': {'': {'vector_count': 115}},\n",
      " 'total_vector_count': 115}\n"
     ]
    }
   ],
   "source": [
    "# initialize pinecone\n",
    "pinecone.init(\n",
    "    api_key=PINECONE_API_KEY,  # find at app.pinecone.io\n",
    "    environment=PINECONE_ENVIRONMENT,  # next to api key in console\n",
    ")\n",
    "\n",
    "index_name = INDEX_NAME\n",
    "\n",
    "embeddings = OpenAIEmbeddings(model='text-embedding-ada-002')\n",
    "# if you already have an index, you can load it like this\n",
    "docsearch = Pinecone.from_existing_index(index_name, embeddings)\n",
    "\n",
    "# List all indexes information\n",
    "index_description = pinecone.describe_index(index_name)\n",
    "print('index_description: ', index_description)\n",
    "\n",
    "index = pinecone.Index(index_name) \n",
    "index_stats_response = index.describe_index_stats()\n",
    "print('index_stats_response: ', index_stats_response)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Create Vectorstore"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "metadata": {},
   "outputs": [],
   "source": [
    "vectorstore = Pinecone(index, embeddings.embed_query, \"text\")"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# OpenAI Function Calling\n",
    "\n",
    "> describe functions to gpt-4-0613 and gpt-3.5-turbo-0613, and have the model intelligently choose to output a JSON object containing arguments to call those functions\n",
    "\n",
    "\n",
    "models have been fine-tuned to both detect when a function needs to be called (depending on the userâ€™s input) and to respond with JSON that adheres to the function signature"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Functions are specified with the following fields:\n",
    "\n",
    "* Name: The name of the function.\n",
    "* Description: A description of what the function does. The model will use this to decide when to call the function.\n",
    "* Parameters: The parameters object contains all of the input fields the function requires. These inputs can be of the following types: String, Number, Boolean, Object, Null, AnyOf. Refer to the API reference docs for details.\n",
    "* Required: Which of the parameters are required to make a query. The rest will be treated as optional."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Create QAretriever"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 7,
   "metadata": {},
   "outputs": [],
   "source": [
    "qa_chain = RetrievalQA.from_chain_type(\n",
    "    llm=llm_qa,\n",
    "    chain_type=\"refine\",\n",
    "    retriever=vectorstore.as_retriever(),\n",
    "    # verbose=True,\n",
    ")\n",
    "print(\"hi\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 9,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "{'query': 'What happened on July 29 in Brazil?',\n",
       " 'result': 'The context does not provide information on what happened on July 29 in Brazil.'}"
      ]
     },
     "execution_count": 9,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "query = \"What happened on July 29 in Brazil?\"\n",
    "response = qa_chain(query)\n",
    "response"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 13,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "{'query': 'What is MaskOCR', 'result': 'MaskOCR is a model pretraining technique for text recognition developed by Baidu Inc. It is designed to recognize texts within a digital image, such as a scanned document or a photo of a document. It has wide-range applications, including visual search and document digitization. \\n\\nThe technique follows a sequence-based pipeline and adopts an encoder-decoder transformer for text recognition. The encoder uses a ViT architecture, a sequence of self-attention and FFN blocks, for text image patch representation extraction. The decoder is formed with the DETR-style decoder, a sequence of self-attention, cross-attention and FFN blocks, mapping the patch representations to a text with an expected role of language modeling.\\n\\nMaskOCR pretrains both the encoder and the decoder in a sequential manner using a masking strategy. The encoder is pretrained using a self-supervised approach, where the text image is divided into a set of vertical patches. Some patches, which may contain parts of characters or whole characters, are randomly masked. The encoder then predicts the representations of these masked patches from the visible patches in the representation space. \\n\\nThe decoder, on the other hand, is pretrained in a supervised manner over synthesized text images. The pretrained encoder is fixed and only the decoder is updated, allowing the pretraining task to explore language rules without being affected by the style of the synthesized text images, which might differ from the downstream tasks.\\n\\nExperiments show that the proposed MaskOCR approach achieves superior results on the benchmark datasets, including Chinese and English text images. The method benefits from larger models, pushing the state-of-the-art results to a new level of 93.8%. \\n\\nThe core of MaskOCR lies in the pretraining of the recognition model, including both the encoder and the decoder. The encoder pretraining is able to benefit from large scale real text images that are easily available without the need for text annotation. The decoder pretraining is able to benefit from the synthetic text images that are also easily available with the character-level annotation easily obtained. This approach verifies the effectiveness of encoder-decoder pretraining.'}\n"
     ]
    }
   ],
   "source": [
    "print(dict(response))"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Prompt Template for Video chain "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 15,
   "metadata": {},
   "outputs": [],
   "source": [
    "examples =[\n",
    "{\n",
    "  \"list_of_scenes\":[\n",
    "    {\n",
    "      \"scene\": \"family trip skiing\",\n",
    "      \"subtitles\": [\n",
    "        \"Are you ready for an unforgettable family ski trip to Japan?\",\n",
    "        \"Ensure your adventure is worry-free with Singlife's travel insurance!\"\n",
    "      ]\n",
    "    },\n",
    "    {\n",
    "      \"scene\": \"insurance policy document close up\",\n",
    "      \"subtitles\": [\n",
    "        \"Our comprehensive travel insurance plans cater to your needs.\",\n",
    "        \"Peace of mind throughout your journey.\"\n",
    "      ]\n",
    "    },\n",
    "    {\n",
    "      \"scene\": \"beach vacation\",\n",
    "      \"subtitles\": [\n",
    "        \"Escape to paradise with our beach vacation package.\",\n",
    "        \"Experience the sun, sand, and surf like never before!\"\n",
    "      ]\n",
    "    },\n",
    "    {\n",
    "      \"scene\": \"exploring ancient ruins\",\n",
    "      \"subtitles\": [\n",
    "        \"Embark on a thrilling adventure to discover ancient ruins.\",\n",
    "        \"Uncover the secrets of the past with Singlife as your travel companion.\"\n",
    "      ]\n",
    "    },\n",
    "    {\n",
    "      \"scene\": \"city sightseeing\",\n",
    "      \"subtitles\": [\n",
    "        \"Immerse yourself in the vibrant energy of a bustling city.\",\n",
    "        \"Explore iconic landmarks and create unforgettable memories.\"\n",
    "      ]\n",
    "    },\n",
    "    {\n",
    "      \"scene\": \"wildlife safari\",\n",
    "      \"subtitles\": [\n",
    "        \"Embark on a wildlife safari and witness nature's wonders up close.\",\n",
    "        \"Let Singlife protect you on your extraordinary journey.\"\n",
    "      ]\n",
    "    }\n",
    "  ]\n",
    "}\n",
    "]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 16,
   "metadata": {},
   "outputs": [],
   "source": [
    "prompt_style =\"Informative and Useful for students to better understand\""
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 26,
   "metadata": {},
   "outputs": [],
   "source": [
    "query = response[\"query\"]\n",
    "result = response[\"result\"]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 29,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "PromptTemplate(input_variables=['result', 'query'], output_parser=None, partial_variables={}, template='Goal:Generate poster summary prompted based on custom knowledge base (Information below) and user query. Two components 1.Scene assets descriptions 2.Subtitle script.\\n\\nCustom knowledge base:{result}\\n\\nUsing the above information, generate a json object that addresses this user query:\\n\\n\"{query}\".\\n\\nExample Output: {\\n  \"title\": [\\n    {\\n      \"What is Algebra\": [\\n        {\\n          \"image\": \"Math teacher teaching math\",\\n          \"text\": \"Algebra is the study of variables and the rules for manipulating these variables in formulas\"\\n        }\\n      ],\\n      \"History of Algebra\": [\\n        {\\n          \"image\": \"Ancient Egyptians doing algebra\",\\n          \"text\": \"The origins of algebra can be traced back to ancient civilizations, including the Babylonians and Egyptians, who developed methods for solving linear and quadratic equations. They used geometric and arithmetic techniques to solve practical problems.\"\\n        }\\n      ],\\n      \"Applications of Algebra\": [\\n        {\\n          \"image\": \"Algebra in Physics\",\\n          \"text\": \"Algebra is extensively used in physics to model and analyze physical phenomena. Equations of motion, electromagnetism, quantum mechanics, and fluid dynamics are expressed using algebraic equations.\"\\n        }\\n      ]\\n    }\\n  ]\\n}\\n\\nReturn the generated json in the style/format: Informative and Useful for students to better understand', template_format='f-string', validate_template=False)"
      ]
     },
     "execution_count": 29,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "# Prompt template for video script generation\n",
    "prompt = PromptTemplate(\n",
    "    template=\"\"\"Goal:Generate poster summary prompted based on custom knowledge base (Information below) and user query. Two components 1.Scene assets descriptions 2.Subtitle script.\\n\\nCustom knowledge base:{result}\\n\\nUsing the above information, generate a json object that addresses this user query:\\n\\n\"{query}\".\\n\\nExample Output: {\n",
    "  \"title\": [\n",
    "    {\n",
    "      \"What is Algebra\": [\n",
    "        {\n",
    "          \"image\": \"Math teacher teaching math\",\n",
    "          \"text\": \"Algebra is the study of variables and the rules for manipulating these variables in formulas\"\n",
    "        }\n",
    "      ],\n",
    "      \"History of Algebra\": [\n",
    "        {\n",
    "          \"image\": \"Ancient Egyptians doing algebra\",\n",
    "          \"text\": \"The origins of algebra can be traced back to ancient civilizations, including the Babylonians and Egyptians, who developed methods for solving linear and quadratic equations. They used geometric and arithmetic techniques to solve practical problems.\"\n",
    "        }\n",
    "      ],\n",
    "      \"Applications of Algebra\": [\n",
    "        {\n",
    "          \"image\": \"Algebra in Physics\",\n",
    "          \"text\": \"Algebra is extensively used in physics to model and analyze physical phenomena. Equations of motion, electromagnetism, quantum mechanics, and fluid dynamics are expressed using algebraic equations.\"\n",
    "        }\n",
    "      ]\n",
    "    }\n",
    "  ]\n",
    "}\n",
    "\n",
    "Return the generated json in the style/format: \"\"\" + prompt_style,\n",
    "    input_variables= [\"result\", \"query\"],\n",
    "    validate_template=False\n",
    ")\n",
    "prompt"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 30,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "'What is MaskOCR'"
      ]
     },
     "execution_count": 30,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "query\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 31,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "PromptTemplate(input_variables=['result', 'query'], output_parser=None, partial_variables={}, template='Goal:Generate 15-30sec video script based on custom knowledge base (Information below) and user query. Two components 1.Scene assets descriptions 2.Subtitle script \\n    Custom knowledge base:{result}\\n\\nUsing the above information, generate a video script that addresses this user query:\\n\\n\"{query}\".\\nReturn the generated video script in the style/format: Informative and Useful for students to better understand', template_format='f-string', validate_template=True)"
      ]
     },
     "execution_count": 31,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "prompt = PromptTemplate(\n",
    "    template=\"\"\"Goal:Generate 15-30sec video script based on custom knowledge base (Information below) and user query. Two components 1.Scene assets descriptions 2.Subtitle script \n",
    "    Custom knowledge base:{result}\\n\\nUsing the above information, generate a video script that addresses this user query:\\n\\n\"{query}\".\\nReturn the generated video script in the style/format: \"\"\"+prompt_style,\n",
    "    input_variables= [\"result\", \"query\"]\n",
    ")\n",
    "prompt"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Create the video Chain"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "json_schema2 = {\n",
    "    \"name\": \"format_prompt\",\n",
    "    \"description\": \"Formats to a 15-30sec video script.\",\n",
    "    \"type\": \"object\",\n",
    "    \"properties\": {\n",
    "      \"list_of_scenes\": {\n",
    "        \"type\": \"array\",\n",
    "        \"items\": {\n",
    "          \"type\": \"object\",\n",
    "          \"properties\": {\n",
    "            \"scene\": {\n",
    "              \"type\": \"string\",\n",
    "              \"description\": \"Scene description for video should be visual and general\"\n",
    "            },\n",
    "            \"subtitles\": {\n",
    "              \"type\": \"array\",\n",
    "              \"items\": {\n",
    "                \"type\": \"string\",\n",
    "                \"description\": \"video subtitles script for video\"\n",
    "              }\n",
    "            }\n",
    "          },\n",
    "          \"required\": [\"scene\", \"subtitles\"]\n",
    "        }\n",
    "      }\n",
    "    },\n",
    "    \"required\": [\"list_of_scenes\"]\n",
    "  }"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# json_schema = {\n",
    "#         \"title\": \"generate_video_script\",\n",
    "#         \"description\": \"Generates 15-30sec video script based on custom knowledge base. Two components 1.Scene descriptions 2.Subtitle script\",\n",
    "#         \"type\": \"object\",\n",
    "#         \"properties\": {\n",
    "#           \"list_of_video_chunk\": {\n",
    "#             \"type\": \"array\",\n",
    "#             \"description\": \"List of video_chunk to be included in the video, one video chunk should last 3-5 seconds and is a dictionary with keys: 1. Scene  2. Subtitle\",\n",
    "#             \"items\": {\n",
    "#               \"type\": \"object\"\n",
    "#             },\n",
    "#             \"properties\": {\n",
    "#               \"scene\": {\n",
    "#                 \"type\": \"string\",\n",
    "#                 \"description\": \"Scene description for video should be visual and general\"\n",
    "#               },\n",
    "#               \"subtitles\": {\n",
    "#                 \"type\": \"string\",\n",
    "#                 \"description\": \"Funny and sarcastic video subtitles script for video\"\n",
    "#               }\n",
    "#             },\n",
    "#             \"required\": [\"scene\", \"subtitles\"]\n",
    "#           }\n",
    "#         },\"required\": [\"list_of_video_chunk\"],\n",
    "#       }\n",
    "# json_schema2 = {\n",
    "#     \"name\": \"format_video_script\",\n",
    "#     \"description\": \"Formats to a 15-30sec video script.\",\n",
    "#     \"type\": \"object\",\n",
    "#     \"properties\": {\n",
    "#       \"list_of_scenes\": {\n",
    "#         \"type\": \"array\",\n",
    "#         \"items\": {\n",
    "#           \"type\": \"object\",\n",
    "#           \"properties\": {\n",
    "#             \"scene\": {\n",
    "#               \"type\": \"string\",\n",
    "#               \"description\": \"Scene description for video should be visual and general\"\n",
    "#             },\n",
    "#             \"subtitles\": {\n",
    "#               \"type\": \"array\",\n",
    "#               \"items\": {\n",
    "#                 \"type\": \"string\",\n",
    "#                 \"description\": \"video subtitles script for video\"\n",
    "#               }\n",
    "#             }\n",
    "#           },\n",
    "#           \"required\": [\"scene\", \"subtitles\"]\n",
    "#         }\n",
    "#       }\n",
    "#     },\n",
    "#     \"required\": [\"list_of_scenes\"]\n",
    "#   }\n",
    "\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "video_chain2 = create_structured_output_chain(json_schema2, llm_qa, prompt, verbose=True)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Chain for video script generation"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "overall_chain = SequentialChain(chains=[qa_chain, video_chain2],input_variables=[\"query\"])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "SequentialChain(memory=None, callbacks=None, callback_manager=None, verbose=False, tags=None, metadata=None, chains=[RetrievalQA(memory=None, callbacks=None, callback_manager=None, verbose=True, tags=None, metadata=None, combine_documents_chain=RefineDocumentsChain(memory=None, callbacks=None, callback_manager=None, verbose=False, tags=None, metadata=None, input_key='input_documents', output_key='output_text', initial_llm_chain=LLMChain(memory=None, callbacks=None, callback_manager=None, verbose=False, tags=None, metadata=None, prompt=ChatPromptTemplate(input_variables=['context_str', 'question'], output_parser=None, partial_variables={}, messages=[SystemMessagePromptTemplate(prompt=PromptTemplate(input_variables=['context_str'], output_parser=None, partial_variables={}, template='Context information is below. \\n---------------------\\n{context_str}\\n---------------------\\nGiven the context information and not prior knowledge, answer any questions', template_format='f-string', validate_template=True), additional_kwargs={}), HumanMessagePromptTemplate(prompt=PromptTemplate(input_variables=['question'], output_parser=None, partial_variables={}, template='{question}', template_format='f-string', validate_template=True), additional_kwargs={})]), llm=ChatOpenAI(cache=None, verbose=False, callbacks=None, callback_manager=None, tags=None, metadata=None, client=<class 'openai.api_resources.chat_completion.ChatCompletion'>, model_name='gpt-4-0613', temperature=0.0, model_kwargs={}, openai_api_key='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', openai_api_base='', openai_organization='', openai_proxy='', request_timeout=None, max_retries=6, streaming=False, n=1, max_tokens=None, tiktoken_model_name=None), output_key='text', output_parser=NoOpOutputParser(), return_final_only=True, llm_kwargs={}), refine_llm_chain=LLMChain(memory=None, callbacks=None, callback_manager=None, verbose=False, tags=None, metadata=None, prompt=ChatPromptTemplate(input_variables=['context_str', 'question', 'existing_answer'], output_parser=None, partial_variables={}, messages=[HumanMessagePromptTemplate(prompt=PromptTemplate(input_variables=['question'], output_parser=None, partial_variables={}, template='{question}', template_format='f-string', validate_template=True), additional_kwargs={}), AIMessagePromptTemplate(prompt=PromptTemplate(input_variables=['existing_answer'], output_parser=None, partial_variables={}, template='{existing_answer}', template_format='f-string', validate_template=True), additional_kwargs={}), HumanMessagePromptTemplate(prompt=PromptTemplate(input_variables=['context_str'], output_parser=None, partial_variables={}, template=\"We have the opportunity to refine the existing answer(only if needed) with some more context below.\\n------------\\n{context_str}\\n------------\\nGiven the new context, refine the original answer to better answer the question. If the context isn't useful, return the original answer.\", template_format='f-string', validate_template=True), additional_kwargs={})]), llm=ChatOpenAI(cache=None, verbose=False, callbacks=None, callback_manager=None, tags=None, metadata=None, client=<class 'openai.api_resources.chat_completion.ChatCompletion'>, model_name='gpt-4-0613', temperature=0.0, model_kwargs={}, openai_api_key='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', openai_api_base='', openai_organization='', openai_proxy='', request_timeout=None, max_retries=6, streaming=False, n=1, max_tokens=None, tiktoken_model_name=None), output_key='text', output_parser=NoOpOutputParser(), return_final_only=True, llm_kwargs={}), document_variable_name='context_str', initial_response_name='existing_answer', document_prompt=PromptTemplate(input_variables=['page_content'], output_parser=None, partial_variables={}, template='{page_content}', template_format='f-string', validate_template=True), return_intermediate_steps=False), input_key='query', output_key='result', return_source_documents=False, retriever=VectorStoreRetriever(tags=None, metadata=None, vectorstore=<langchain.vectorstores.pinecone.Pinecone object at 0x00000242E91B21D0>, search_type='similarity', search_kwargs={})), LLMChain(memory=None, callbacks=None, callback_manager=None, verbose=True, tags=None, metadata=None, prompt=PromptTemplate(input_variables=['result', 'query'], output_parser=None, partial_variables={}, template='Goal:Generate 15-30sec video script based on custom knowledge base (Information below) and user query. Two components 1.Scene assets descriptions 2.Subtitle script.\\n\\nCustom knowledge base:{result}\\n\\nUsing the above information, generate a video script that addresses this user query:\\n\\n\"{query}\".\\n\\nExample Output: {\\n  \"list_of_scenes\":[\\n    {\\n      \"scene\": \"family trip skiing\",\\n      \"subtitles\": [\\n        \"Are you ready for an unforgettable family ski trip to Japan?\",\\n        \"Ensure your adventure is worry-free with Singlife\\'s travel insurance!\"\\n      ]\\n    },\\n    {\\n      \"scene\": \"insurance policy document close up\",\\n      \"subtitles\": [\\n        \"Our comprehensive travel insurance plans cater to your needs.\",\\n        \"Peace of mind throughout your journey.\"\\n      ]\\n    },\\n    {\\n      \"scene\": \"beach vacation\",\\n      \"subtitles\": [\\n        \"Escape to paradise with our beach vacation package.\",\\n        \"Experience the sun, sand, and surf like never before!\"\\n      ]\\n    },\\n    {\\n      \"scene\": \"exploring ancient ruins\",\\n      \"subtitles\": [\\n        \"Embark on a thrilling adventure to discover ancient ruins.\",\\n        \"Uncover the secrets of the past with Singlife as your travel companion.\"\\n      ]\\n    },\\n    {\\n      \"scene\": \"city sightseeing\",\\n      \"subtitles\": [\\n        \"Immerse yourself in the vibrant energy of a bustling city.\",\\n        \"Explore iconic landmarks and create unforgettable memories.\"\\n      ]\\n    },\\n    {\\n      \"scene\": \"wildlife safari\",\\n      \"subtitles\": [\\n        \"Embark on a wildlife safari and witness nature\\'s wonders up close.\",\\n        \"Let Singlife protect you on your extraordinary journey.\"\\n      ]\\n    }\\n  ]\\n}\\nReturn the generated video script in the style/format: Funny and sarcastic', template_format='f-string', validate_template=False), llm=ChatOpenAI(cache=None, verbose=False, callbacks=None, callback_manager=None, tags=None, metadata=None, client=<class 'openai.api_resources.chat_completion.ChatCompletion'>, model_name='gpt-4-0613', temperature=0.0, model_kwargs={}, openai_api_key='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', openai_api_base='', openai_organization='', openai_proxy='', request_timeout=None, max_retries=6, streaming=False, n=1, max_tokens=None, tiktoken_model_name=None), output_key='function', output_parser=JsonOutputFunctionsParser(args_only=True), return_final_only=True, llm_kwargs={'functions': [{'name': 'output_formatter', 'description': 'Output formatter. Should always be used to format your response to the user.', 'parameters': {'name': 'format_video_script', 'description': 'Formats to a 15-30sec video script.', 'type': 'object', 'properties': {'list_of_scenes': {'type': 'array', 'items': {'type': 'object', 'properties': {'scene': {'type': 'string', 'description': 'Scene description for video should be visual and general'}, 'subtitles': {'type': 'array', 'items': {'type': 'string', 'description': 'video subtitles script for video'}}}, 'required': ['scene', 'subtitles']}}}, 'required': ['list_of_scenes']}}], 'function_call': {'name': 'output_formatter'}})], input_variables=['query'], output_variables=['function'], return_all=False)"
      ]
     },
     "execution_count": 47,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "overall_chain"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "\n",
      "\n",
      "\u001b[1m> Entering new RetrievalQA chain...\u001b[0m\n",
      "\n",
      "\u001b[1m> Finished chain.\u001b[0m\n",
      "\n",
      "\n",
      "\u001b[1m> Entering new LLMChain chain...\u001b[0m\n"
     ]
    },
    {
     "ename": "KeyError",
     "evalue": "'\\n  \"list_of_scenes\"'",
     "output_type": "error",
     "traceback": [
      "\u001b[1;31m---------------------------------------------------------------------------\u001b[0m",
      "\u001b[1;31mKeyError\u001b[0m                                  Traceback (most recent call last)",
      "Cell \u001b[1;32mIn[51], line 1\u001b[0m\n\u001b[1;32m----> 1\u001b[0m response \u001b[39m=\u001b[39m overall_chain\u001b[39m.\u001b[39;49mrun(query\u001b[39m=\u001b[39;49mquery,callbacks\u001b[39m=\u001b[39;49m[Tracer])\n\u001b[0;32m      2\u001b[0m response\n",
      "File \u001b[1;32mc:\\Users\\51703\\Documents\\GitHub\\SingLife-Polyfintech2023\\env\\lib\\site-packages\\langchain\\chains\\base.py:445\u001b[0m, in \u001b[0;36mChain.run\u001b[1;34m(self, callbacks, tags, metadata, *args, **kwargs)\u001b[0m\n\u001b[0;32m    440\u001b[0m     \u001b[39mreturn\u001b[39;00m \u001b[39mself\u001b[39m(args[\u001b[39m0\u001b[39m], callbacks\u001b[39m=\u001b[39mcallbacks, tags\u001b[39m=\u001b[39mtags, metadata\u001b[39m=\u001b[39mmetadata)[\n\u001b[0;32m    441\u001b[0m         _output_key\n\u001b[0;32m    442\u001b[0m     ]\n\u001b[0;32m    444\u001b[0m \u001b[39mif\u001b[39;00m kwargs \u001b[39mand\u001b[39;00m \u001b[39mnot\u001b[39;00m args:\n\u001b[1;32m--> 445\u001b[0m     \u001b[39mreturn\u001b[39;00m \u001b[39mself\u001b[39;49m(kwargs, callbacks\u001b[39m=\u001b[39;49mcallbacks, tags\u001b[39m=\u001b[39;49mtags, metadata\u001b[39m=\u001b[39;49mmetadata)[\n\u001b[0;32m    446\u001b[0m         _output_key\n\u001b[0;32m    447\u001b[0m     ]\n\u001b[0;32m    449\u001b[0m \u001b[39mif\u001b[39;00m \u001b[39mnot\u001b[39;00m kwargs \u001b[39mand\u001b[39;00m \u001b[39mnot\u001b[39;00m args:\n\u001b[0;32m    450\u001b[0m     \u001b[39mraise\u001b[39;00m \u001b[39mValueError\u001b[39;00m(\n\u001b[0;32m    451\u001b[0m         \u001b[39m\"\u001b[39m\u001b[39m`run` supported with either positional arguments or keyword arguments,\u001b[39m\u001b[39m\"\u001b[39m\n\u001b[0;32m    452\u001b[0m         \u001b[39m\"\u001b[39m\u001b[39m but none were provided.\u001b[39m\u001b[39m\"\u001b[39m\n\u001b[0;32m    453\u001b[0m     )\n",
      "File \u001b[1;32mc:\\Users\\51703\\Documents\\GitHub\\SingLife-Polyfintech2023\\env\\lib\\site-packages\\langchain\\chains\\base.py:243\u001b[0m, in \u001b[0;36mChain.__call__\u001b[1;34m(self, inputs, return_only_outputs, callbacks, tags, metadata, include_run_info)\u001b[0m\n\u001b[0;32m    241\u001b[0m \u001b[39mexcept\u001b[39;00m (\u001b[39mKeyboardInterrupt\u001b[39;00m, \u001b[39mException\u001b[39;00m) \u001b[39mas\u001b[39;00m e:\n\u001b[0;32m    242\u001b[0m     run_manager\u001b[39m.\u001b[39mon_chain_error(e)\n\u001b[1;32m--> 243\u001b[0m     \u001b[39mraise\u001b[39;00m e\n\u001b[0;32m    244\u001b[0m run_manager\u001b[39m.\u001b[39mon_chain_end(outputs)\n\u001b[0;32m    245\u001b[0m final_outputs: Dict[\u001b[39mstr\u001b[39m, Any] \u001b[39m=\u001b[39m \u001b[39mself\u001b[39m\u001b[39m.\u001b[39mprep_outputs(\n\u001b[0;32m    246\u001b[0m     inputs, outputs, return_only_outputs\n\u001b[0;32m    247\u001b[0m )\n",
      "File \u001b[1;32mc:\\Users\\51703\\Documents\\GitHub\\SingLife-Polyfintech2023\\env\\lib\\site-packages\\langchain\\chains\\base.py:237\u001b[0m, in \u001b[0;36mChain.__call__\u001b[1;34m(self, inputs, return_only_outputs, callbacks, tags, metadata, include_run_info)\u001b[0m\n\u001b[0;32m    231\u001b[0m run_manager \u001b[39m=\u001b[39m callback_manager\u001b[39m.\u001b[39mon_chain_start(\n\u001b[0;32m    232\u001b[0m     dumpd(\u001b[39mself\u001b[39m),\n\u001b[0;32m    233\u001b[0m     inputs,\n\u001b[0;32m    234\u001b[0m )\n\u001b[0;32m    235\u001b[0m \u001b[39mtry\u001b[39;00m:\n\u001b[0;32m    236\u001b[0m     outputs \u001b[39m=\u001b[39m (\n\u001b[1;32m--> 237\u001b[0m         \u001b[39mself\u001b[39;49m\u001b[39m.\u001b[39;49m_call(inputs, run_manager\u001b[39m=\u001b[39;49mrun_manager)\n\u001b[0;32m    238\u001b[0m         \u001b[39mif\u001b[39;00m new_arg_supported\n\u001b[0;32m    239\u001b[0m         \u001b[39melse\u001b[39;00m \u001b[39mself\u001b[39m\u001b[39m.\u001b[39m_call(inputs)\n\u001b[0;32m    240\u001b[0m     )\n\u001b[0;32m    241\u001b[0m \u001b[39mexcept\u001b[39;00m (\u001b[39mKeyboardInterrupt\u001b[39;00m, \u001b[39mException\u001b[39;00m) \u001b[39mas\u001b[39;00m e:\n\u001b[0;32m    242\u001b[0m     run_manager\u001b[39m.\u001b[39mon_chain_error(e)\n",
      "File \u001b[1;32mc:\\Users\\51703\\Documents\\GitHub\\SingLife-Polyfintech2023\\env\\lib\\site-packages\\langchain\\chains\\sequential.py:105\u001b[0m, in \u001b[0;36mSequentialChain._call\u001b[1;34m(self, inputs, run_manager)\u001b[0m\n\u001b[0;32m    103\u001b[0m \u001b[39mfor\u001b[39;00m i, chain \u001b[39min\u001b[39;00m \u001b[39menumerate\u001b[39m(\u001b[39mself\u001b[39m\u001b[39m.\u001b[39mchains):\n\u001b[0;32m    104\u001b[0m     callbacks \u001b[39m=\u001b[39m _run_manager\u001b[39m.\u001b[39mget_child()\n\u001b[1;32m--> 105\u001b[0m     outputs \u001b[39m=\u001b[39m chain(known_values, return_only_outputs\u001b[39m=\u001b[39;49m\u001b[39mTrue\u001b[39;49;00m, callbacks\u001b[39m=\u001b[39;49mcallbacks)\n\u001b[0;32m    106\u001b[0m     known_values\u001b[39m.\u001b[39mupdate(outputs)\n\u001b[0;32m    107\u001b[0m \u001b[39mreturn\u001b[39;00m {k: known_values[k] \u001b[39mfor\u001b[39;00m k \u001b[39min\u001b[39;00m \u001b[39mself\u001b[39m\u001b[39m.\u001b[39moutput_variables}\n",
      "File \u001b[1;32mc:\\Users\\51703\\Documents\\GitHub\\SingLife-Polyfintech2023\\env\\lib\\site-packages\\langchain\\chains\\base.py:243\u001b[0m, in \u001b[0;36mChain.__call__\u001b[1;34m(self, inputs, return_only_outputs, callbacks, tags, metadata, include_run_info)\u001b[0m\n\u001b[0;32m    241\u001b[0m \u001b[39mexcept\u001b[39;00m (\u001b[39mKeyboardInterrupt\u001b[39;00m, \u001b[39mException\u001b[39;00m) \u001b[39mas\u001b[39;00m e:\n\u001b[0;32m    242\u001b[0m     run_manager\u001b[39m.\u001b[39mon_chain_error(e)\n\u001b[1;32m--> 243\u001b[0m     \u001b[39mraise\u001b[39;00m e\n\u001b[0;32m    244\u001b[0m run_manager\u001b[39m.\u001b[39mon_chain_end(outputs)\n\u001b[0;32m    245\u001b[0m final_outputs: Dict[\u001b[39mstr\u001b[39m, Any] \u001b[39m=\u001b[39m \u001b[39mself\u001b[39m\u001b[39m.\u001b[39mprep_outputs(\n\u001b[0;32m    246\u001b[0m     inputs, outputs, return_only_outputs\n\u001b[0;32m    247\u001b[0m )\n",
      "File \u001b[1;32mc:\\Users\\51703\\Documents\\GitHub\\SingLife-Polyfintech2023\\env\\lib\\site-packages\\langchain\\chains\\base.py:237\u001b[0m, in \u001b[0;36mChain.__call__\u001b[1;34m(self, inputs, return_only_outputs, callbacks, tags, metadata, include_run_info)\u001b[0m\n\u001b[0;32m    231\u001b[0m run_manager \u001b[39m=\u001b[39m callback_manager\u001b[39m.\u001b[39mon_chain_start(\n\u001b[0;32m    232\u001b[0m     dumpd(\u001b[39mself\u001b[39m),\n\u001b[0;32m    233\u001b[0m     inputs,\n\u001b[0;32m    234\u001b[0m )\n\u001b[0;32m    235\u001b[0m \u001b[39mtry\u001b[39;00m:\n\u001b[0;32m    236\u001b[0m     outputs \u001b[39m=\u001b[39m (\n\u001b[1;32m--> 237\u001b[0m         \u001b[39mself\u001b[39;49m\u001b[39m.\u001b[39;49m_call(inputs, run_manager\u001b[39m=\u001b[39;49mrun_manager)\n\u001b[0;32m    238\u001b[0m         \u001b[39mif\u001b[39;00m new_arg_supported\n\u001b[0;32m    239\u001b[0m         \u001b[39melse\u001b[39;00m \u001b[39mself\u001b[39m\u001b[39m.\u001b[39m_call(inputs)\n\u001b[0;32m    240\u001b[0m     )\n\u001b[0;32m    241\u001b[0m \u001b[39mexcept\u001b[39;00m (\u001b[39mKeyboardInterrupt\u001b[39;00m, \u001b[39mException\u001b[39;00m) \u001b[39mas\u001b[39;00m e:\n\u001b[0;32m    242\u001b[0m     run_manager\u001b[39m.\u001b[39mon_chain_error(e)\n",
      "File \u001b[1;32mc:\\Users\\51703\\Documents\\GitHub\\SingLife-Polyfintech2023\\env\\lib\\site-packages\\langchain\\chains\\llm.py:92\u001b[0m, in \u001b[0;36mLLMChain._call\u001b[1;34m(self, inputs, run_manager)\u001b[0m\n\u001b[0;32m     87\u001b[0m \u001b[39mdef\u001b[39;00m \u001b[39m_call\u001b[39m(\n\u001b[0;32m     88\u001b[0m     \u001b[39mself\u001b[39m,\n\u001b[0;32m     89\u001b[0m     inputs: Dict[\u001b[39mstr\u001b[39m, Any],\n\u001b[0;32m     90\u001b[0m     run_manager: Optional[CallbackManagerForChainRun] \u001b[39m=\u001b[39m \u001b[39mNone\u001b[39;00m,\n\u001b[0;32m     91\u001b[0m ) \u001b[39m-\u001b[39m\u001b[39m>\u001b[39m Dict[\u001b[39mstr\u001b[39m, \u001b[39mstr\u001b[39m]:\n\u001b[1;32m---> 92\u001b[0m     response \u001b[39m=\u001b[39m \u001b[39mself\u001b[39;49m\u001b[39m.\u001b[39;49mgenerate([inputs], run_manager\u001b[39m=\u001b[39;49mrun_manager)\n\u001b[0;32m     93\u001b[0m     \u001b[39mreturn\u001b[39;00m \u001b[39mself\u001b[39m\u001b[39m.\u001b[39mcreate_outputs(response)[\u001b[39m0\u001b[39m]\n",
      "File \u001b[1;32mc:\\Users\\51703\\Documents\\GitHub\\SingLife-Polyfintech2023\\env\\lib\\site-packages\\langchain\\chains\\llm.py:101\u001b[0m, in \u001b[0;36mLLMChain.generate\u001b[1;34m(self, input_list, run_manager)\u001b[0m\n\u001b[0;32m     95\u001b[0m \u001b[39mdef\u001b[39;00m \u001b[39mgenerate\u001b[39m(\n\u001b[0;32m     96\u001b[0m     \u001b[39mself\u001b[39m,\n\u001b[0;32m     97\u001b[0m     input_list: List[Dict[\u001b[39mstr\u001b[39m, Any]],\n\u001b[0;32m     98\u001b[0m     run_manager: Optional[CallbackManagerForChainRun] \u001b[39m=\u001b[39m \u001b[39mNone\u001b[39;00m,\n\u001b[0;32m     99\u001b[0m ) \u001b[39m-\u001b[39m\u001b[39m>\u001b[39m LLMResult:\n\u001b[0;32m    100\u001b[0m \u001b[39m    \u001b[39m\u001b[39m\"\"\"Generate LLM result from inputs.\"\"\"\u001b[39;00m\n\u001b[1;32m--> 101\u001b[0m     prompts, stop \u001b[39m=\u001b[39m \u001b[39mself\u001b[39;49m\u001b[39m.\u001b[39;49mprep_prompts(input_list, run_manager\u001b[39m=\u001b[39;49mrun_manager)\n\u001b[0;32m    102\u001b[0m     \u001b[39mreturn\u001b[39;00m \u001b[39mself\u001b[39m\u001b[39m.\u001b[39mllm\u001b[39m.\u001b[39mgenerate_prompt(\n\u001b[0;32m    103\u001b[0m         prompts,\n\u001b[0;32m    104\u001b[0m         stop,\n\u001b[0;32m    105\u001b[0m         callbacks\u001b[39m=\u001b[39mrun_manager\u001b[39m.\u001b[39mget_child() \u001b[39mif\u001b[39;00m run_manager \u001b[39melse\u001b[39;00m \u001b[39mNone\u001b[39;00m,\n\u001b[0;32m    106\u001b[0m         \u001b[39m*\u001b[39m\u001b[39m*\u001b[39m\u001b[39mself\u001b[39m\u001b[39m.\u001b[39mllm_kwargs,\n\u001b[0;32m    107\u001b[0m     )\n",
      "File \u001b[1;32mc:\\Users\\51703\\Documents\\GitHub\\SingLife-Polyfintech2023\\env\\lib\\site-packages\\langchain\\chains\\llm.py:135\u001b[0m, in \u001b[0;36mLLMChain.prep_prompts\u001b[1;34m(self, input_list, run_manager)\u001b[0m\n\u001b[0;32m    133\u001b[0m \u001b[39mfor\u001b[39;00m inputs \u001b[39min\u001b[39;00m input_list:\n\u001b[0;32m    134\u001b[0m     selected_inputs \u001b[39m=\u001b[39m {k: inputs[k] \u001b[39mfor\u001b[39;00m k \u001b[39min\u001b[39;00m \u001b[39mself\u001b[39m\u001b[39m.\u001b[39mprompt\u001b[39m.\u001b[39minput_variables}\n\u001b[1;32m--> 135\u001b[0m     prompt \u001b[39m=\u001b[39m \u001b[39mself\u001b[39m\u001b[39m.\u001b[39mprompt\u001b[39m.\u001b[39mformat_prompt(\u001b[39m*\u001b[39m\u001b[39m*\u001b[39mselected_inputs)\n\u001b[0;32m    136\u001b[0m     _colored_text \u001b[39m=\u001b[39m get_colored_text(prompt\u001b[39m.\u001b[39mto_string(), \u001b[39m\"\u001b[39m\u001b[39mgreen\u001b[39m\u001b[39m\"\u001b[39m)\n\u001b[0;32m    137\u001b[0m     _text \u001b[39m=\u001b[39m \u001b[39m\"\u001b[39m\u001b[39mPrompt after formatting:\u001b[39m\u001b[39m\\n\u001b[39;00m\u001b[39m\"\u001b[39m \u001b[39m+\u001b[39m _colored_text\n",
      "File \u001b[1;32mc:\\Users\\51703\\Documents\\GitHub\\SingLife-Polyfintech2023\\env\\lib\\site-packages\\langchain\\prompts\\base.py:114\u001b[0m, in \u001b[0;36mStringPromptTemplate.format_prompt\u001b[1;34m(self, **kwargs)\u001b[0m\n\u001b[0;32m    112\u001b[0m \u001b[39mdef\u001b[39;00m \u001b[39mformat_prompt\u001b[39m(\u001b[39mself\u001b[39m, \u001b[39m*\u001b[39m\u001b[39m*\u001b[39mkwargs: Any) \u001b[39m-\u001b[39m\u001b[39m>\u001b[39m PromptValue:\n\u001b[0;32m    113\u001b[0m \u001b[39m    \u001b[39m\u001b[39m\"\"\"Create Chat Messages.\"\"\"\u001b[39;00m\n\u001b[1;32m--> 114\u001b[0m     \u001b[39mreturn\u001b[39;00m StringPromptValue(text\u001b[39m=\u001b[39m\u001b[39mself\u001b[39m\u001b[39m.\u001b[39mformat(\u001b[39m*\u001b[39m\u001b[39m*\u001b[39mkwargs))\n",
      "File \u001b[1;32mc:\\Users\\51703\\Documents\\GitHub\\SingLife-Polyfintech2023\\env\\lib\\site-packages\\langchain\\prompts\\prompt.py:67\u001b[0m, in \u001b[0;36mPromptTemplate.format\u001b[1;34m(self, **kwargs)\u001b[0m\n\u001b[0;32m     52\u001b[0m \u001b[39m\u001b[39m\u001b[39m\"\"\"Format the prompt with the inputs.\u001b[39;00m\n\u001b[0;32m     53\u001b[0m \n\u001b[0;32m     54\u001b[0m \u001b[39mArgs:\u001b[39;00m\n\u001b[1;32m   (...)\u001b[0m\n\u001b[0;32m     64\u001b[0m \u001b[39m    prompt.format(variable1=\"foo\")\u001b[39;00m\n\u001b[0;32m     65\u001b[0m \u001b[39m\"\"\"\u001b[39;00m\n\u001b[0;32m     66\u001b[0m kwargs \u001b[39m=\u001b[39m \u001b[39mself\u001b[39m\u001b[39m.\u001b[39m_merge_partial_and_user_variables(\u001b[39m*\u001b[39m\u001b[39m*\u001b[39mkwargs)\n\u001b[1;32m---> 67\u001b[0m \u001b[39mreturn\u001b[39;00m DEFAULT_FORMATTER_MAPPING[\u001b[39mself\u001b[39m\u001b[39m.\u001b[39mtemplate_format](\u001b[39mself\u001b[39m\u001b[39m.\u001b[39mtemplate, \u001b[39m*\u001b[39m\u001b[39m*\u001b[39mkwargs)\n",
      "File \u001b[1;32m~\\AppData\\Local\\Programs\\Python\\Python310\\lib\\string.py:161\u001b[0m, in \u001b[0;36mFormatter.format\u001b[1;34m(self, format_string, *args, **kwargs)\u001b[0m\n\u001b[0;32m    160\u001b[0m \u001b[39mdef\u001b[39;00m \u001b[39mformat\u001b[39m(\u001b[39mself\u001b[39m, format_string, \u001b[39m/\u001b[39m, \u001b[39m*\u001b[39margs, \u001b[39m*\u001b[39m\u001b[39m*\u001b[39mkwargs):\n\u001b[1;32m--> 161\u001b[0m     \u001b[39mreturn\u001b[39;00m \u001b[39mself\u001b[39;49m\u001b[39m.\u001b[39;49mvformat(format_string, args, kwargs)\n",
      "File \u001b[1;32mc:\\Users\\51703\\Documents\\GitHub\\SingLife-Polyfintech2023\\env\\lib\\site-packages\\langchain\\formatting.py:29\u001b[0m, in \u001b[0;36mStrictFormatter.vformat\u001b[1;34m(self, format_string, args, kwargs)\u001b[0m\n\u001b[0;32m     24\u001b[0m \u001b[39mif\u001b[39;00m \u001b[39mlen\u001b[39m(args) \u001b[39m>\u001b[39m \u001b[39m0\u001b[39m:\n\u001b[0;32m     25\u001b[0m     \u001b[39mraise\u001b[39;00m \u001b[39mValueError\u001b[39;00m(\n\u001b[0;32m     26\u001b[0m         \u001b[39m\"\u001b[39m\u001b[39mNo arguments should be provided, \u001b[39m\u001b[39m\"\u001b[39m\n\u001b[0;32m     27\u001b[0m         \u001b[39m\"\u001b[39m\u001b[39meverything should be passed as keyword arguments.\u001b[39m\u001b[39m\"\u001b[39m\n\u001b[0;32m     28\u001b[0m     )\n\u001b[1;32m---> 29\u001b[0m \u001b[39mreturn\u001b[39;00m \u001b[39msuper\u001b[39;49m()\u001b[39m.\u001b[39;49mvformat(format_string, args, kwargs)\n",
      "File \u001b[1;32m~\\AppData\\Local\\Programs\\Python\\Python310\\lib\\string.py:165\u001b[0m, in \u001b[0;36mFormatter.vformat\u001b[1;34m(self, format_string, args, kwargs)\u001b[0m\n\u001b[0;32m    163\u001b[0m \u001b[39mdef\u001b[39;00m \u001b[39mvformat\u001b[39m(\u001b[39mself\u001b[39m, format_string, args, kwargs):\n\u001b[0;32m    164\u001b[0m     used_args \u001b[39m=\u001b[39m \u001b[39mset\u001b[39m()\n\u001b[1;32m--> 165\u001b[0m     result, _ \u001b[39m=\u001b[39m \u001b[39mself\u001b[39;49m\u001b[39m.\u001b[39;49m_vformat(format_string, args, kwargs, used_args, \u001b[39m2\u001b[39;49m)\n\u001b[0;32m    166\u001b[0m     \u001b[39mself\u001b[39m\u001b[39m.\u001b[39mcheck_unused_args(used_args, args, kwargs)\n\u001b[0;32m    167\u001b[0m     \u001b[39mreturn\u001b[39;00m result\n",
      "File \u001b[1;32m~\\AppData\\Local\\Programs\\Python\\Python310\\lib\\string.py:205\u001b[0m, in \u001b[0;36mFormatter._vformat\u001b[1;34m(self, format_string, args, kwargs, used_args, recursion_depth, auto_arg_index)\u001b[0m\n\u001b[0;32m    201\u001b[0m     auto_arg_index \u001b[39m=\u001b[39m \u001b[39mFalse\u001b[39;00m\n\u001b[0;32m    203\u001b[0m \u001b[39m# given the field_name, find the object it references\u001b[39;00m\n\u001b[0;32m    204\u001b[0m \u001b[39m#  and the argument it came from\u001b[39;00m\n\u001b[1;32m--> 205\u001b[0m obj, arg_used \u001b[39m=\u001b[39m \u001b[39mself\u001b[39;49m\u001b[39m.\u001b[39;49mget_field(field_name, args, kwargs)\n\u001b[0;32m    206\u001b[0m used_args\u001b[39m.\u001b[39madd(arg_used)\n\u001b[0;32m    208\u001b[0m \u001b[39m# do any conversion on the resulting object\u001b[39;00m\n",
      "File \u001b[1;32m~\\AppData\\Local\\Programs\\Python\\Python310\\lib\\string.py:270\u001b[0m, in \u001b[0;36mFormatter.get_field\u001b[1;34m(self, field_name, args, kwargs)\u001b[0m\n\u001b[0;32m    267\u001b[0m \u001b[39mdef\u001b[39;00m \u001b[39mget_field\u001b[39m(\u001b[39mself\u001b[39m, field_name, args, kwargs):\n\u001b[0;32m    268\u001b[0m     first, rest \u001b[39m=\u001b[39m _string\u001b[39m.\u001b[39mformatter_field_name_split(field_name)\n\u001b[1;32m--> 270\u001b[0m     obj \u001b[39m=\u001b[39m \u001b[39mself\u001b[39;49m\u001b[39m.\u001b[39;49mget_value(first, args, kwargs)\n\u001b[0;32m    272\u001b[0m     \u001b[39m# loop through the rest of the field_name, doing\u001b[39;00m\n\u001b[0;32m    273\u001b[0m     \u001b[39m#  getattr or getitem as needed\u001b[39;00m\n\u001b[0;32m    274\u001b[0m     \u001b[39mfor\u001b[39;00m is_attr, i \u001b[39min\u001b[39;00m rest:\n",
      "File \u001b[1;32m~\\AppData\\Local\\Programs\\Python\\Python310\\lib\\string.py:227\u001b[0m, in \u001b[0;36mFormatter.get_value\u001b[1;34m(self, key, args, kwargs)\u001b[0m\n\u001b[0;32m    225\u001b[0m     \u001b[39mreturn\u001b[39;00m args[key]\n\u001b[0;32m    226\u001b[0m \u001b[39melse\u001b[39;00m:\n\u001b[1;32m--> 227\u001b[0m     \u001b[39mreturn\u001b[39;00m kwargs[key]\n",
      "\u001b[1;31mKeyError\u001b[0m: '\\n  \"list_of_scenes\"'"
     ]
    }
   ],
   "source": [
    "response = overall_chain.run(query=query,callbacks=[Tracer])\n",
    "response"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Let the experiment begin"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "query = \"I am travelling to Japan for a ski trip with my family next week.What kind of travel insurance coverage do we need?\""
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [
    {
     "ename": "AttributeError",
     "evalue": "'ChatOpenAI' object has no attribute 'run'",
     "output_type": "error",
     "traceback": [
      "\u001b[1;31m---------------------------------------------------------------------------\u001b[0m",
      "\u001b[1;31mAttributeError\u001b[0m                            Traceback (most recent call last)",
      "Cell \u001b[1;32mIn[253], line 17\u001b[0m\n\u001b[0;32m     15\u001b[0m \u001b[39m# wandb trace\u001b[39;00m\n\u001b[0;32m     16\u001b[0m \u001b[39mwith\u001b[39;00m wandb_tracing_enabled():\n\u001b[1;32m---> 17\u001b[0m     first_response \u001b[39m=\u001b[39m llm_video_script\u001b[39m.\u001b[39;49mrun(result\u001b[39m=\u001b[39muser_request, query\u001b[39m=\u001b[39mquery)\n\u001b[0;32m     19\u001b[0m first_response\n",
      "\u001b[1;31mAttributeError\u001b[0m: 'ChatOpenAI' object has no attribute 'run'"
     ]
    }
   ],
   "source": [
    "user_request = \"\"\"Based on the Singlife Corporate Travel Insurance policy, you should consider the following coverage for your ski trip to Japan:\n",
    "1. Accidental death, permanent disablement and burns benefit: This provides coverage in case of accidental injury during your trip.\n",
    "2. Medical and medical evacuation: This is crucial for a ski trip as it covers any medical emergencies or injuries that may occur, including the cost of evacuation if necessary.\n",
    "3. Trip cancellation: If there's a last-minute cancellation, you can receive coverage for non-refundable deposits or unused travel and accommodation costs.\n",
    "4. Full terrorism cover: This offers a lump-sum payout if an unfortunate event occurs.\n",
    "5. Delayed departure, missed departure or connection: This covers any additional expenses incurred due to delayed or missed flights.\n",
    "6. Loss or damage of baggage and personal belongings: This provides coverage for lost or damaged personal items during your trip.\n",
    "7. Rental vehicle excess: If you plan to rent a vehicle during your trip, this coverage can be beneficial.\n",
    "8. COVID-19 Coverage: The policy covers trip interruptions or cancellations due to COVID-19 and covers the medical treatment if you contract COVID-19 during or after the trip.\n",
    "Remember to choose a plan that best suits your family's needs. The Elite Plan offers comprehensive coverage for frequent travellers, while the Classic Plan covers the basics.\n",
    "\n",
    "Using the above information, generate a video script that addresses this user query:I am travelling to Japan for a ski \n",
    "trip with my family next week. What kind of travel insurance coverage do we need? In the format of sarcastic and funny.\"\"\"\n",
    "\n",
    "# wandb trace\n",
    "with wandb_tracing_enabled():\n",
    "    first_response = llm_video_script\n",
    "\n",
    "first_response"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Documentation testing ( Delete kalate )"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "from langchain.chains.openai_functions.openapi import get_openapi_chain"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Attempting to load an OpenAPI 3.0.1 spec.  This may result in degraded performance. Convert your OpenAPI spec to 3.1.* spec for better support.\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "call_api_fn:  <function openapi_spec_to_openai_fn.<locals>.default_call_api at 0x000001D6B11C8550>\n",
      "openai_fns:  [{'name': 'productsUsingGET', 'description': 'API for fetching Klarna product information', 'parameters': {'type': 'object', 'properties': {'params': {'type': 'object', 'properties': {'countryCode': {'type': 'string', 'description': 'ISO 3166 country code with 2 characters based on the user location. Currently, only US, GB, DE, SE and DK are supported.'}, 'q': {'type': 'string', 'description': \"A precise query that matches one very small category or product that needs to be searched for to find the products the user is looking for. If the user explicitly stated what they want, use that as a query. The query is as specific as possible to the product name or category mentioned by the user in its singular form, and don't contain any clarifiers like latest, newest, cheapest, budget, premium, expensive or similar. The query is always taken from the latest topic, if there is a new topic a new query is started. If the user speaks another language than English, translate their request into English (example: translate fia med knuff to ludo board game)!\"}, 'size': {'type': 'integer', 'description': 'number of products returned'}, 'min_price': {'type': 'integer', 'description': \"(Optional) Minimum price in local currency for the product searched for. Either explicitly stated by the user or implicitly inferred from a combination of the user's request and the kind of product searched for.\"}, 'max_price': {'type': 'integer', 'description': \"(Optional) Maximum price in local currency for the product searched for. Either explicitly stated by the user or implicitly inferred from a combination of the user's request and the kind of product searched for.\"}}, 'required': ['countryCode', 'q']}}}}]\n",
      "prompt: input_variables=['query'] output_parser=None partial_variables={} messages=[HumanMessagePromptTemplate(prompt=PromptTemplate(input_variables=['query'], output_parser=None, partial_variables={}, template=\"Use the provided API's to respond to this user query:\\n\\n{query}\", template_format='f-string', validate_template=True), additional_kwargs={})]\n",
      "output_parser: args_only=False\n"
     ]
    },
    {
     "data": {
      "text/plain": [
       "SequentialChain(memory=None, callbacks=None, callback_manager=None, verbose=False, tags=None, metadata=None, chains=[LLMChain(memory=None, callbacks=None, callback_manager=None, verbose=False, tags=None, metadata=None, prompt=ChatPromptTemplate(input_variables=['query'], output_parser=None, partial_variables={}, messages=[HumanMessagePromptTemplate(prompt=PromptTemplate(input_variables=['query'], output_parser=None, partial_variables={}, template=\"Use the provided API's to respond to this user query:\\n\\n{query}\", template_format='f-string', validate_template=True), additional_kwargs={})]), llm=ChatOpenAI(cache=None, verbose=False, callbacks=None, callback_manager=None, tags=None, metadata=None, client=<class 'openai.api_resources.chat_completion.ChatCompletion'>, model_name='gpt-3.5-turbo-0613', temperature=0.7, model_kwargs={}, openai_api_key='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', openai_api_base='', openai_organization='', openai_proxy='', request_timeout=None, max_retries=6, streaming=False, n=1, max_tokens=None, tiktoken_model_name=None), output_key='function', output_parser=JsonOutputFunctionsParser(args_only=False), return_final_only=True, llm_kwargs={'functions': [{'name': 'productsUsingGET', 'description': 'API for fetching Klarna product information', 'parameters': {'type': 'object', 'properties': {'params': {'type': 'object', 'properties': {'countryCode': {'type': 'string', 'description': 'ISO 3166 country code with 2 characters based on the user location. Currently, only US, GB, DE, SE and DK are supported.'}, 'q': {'type': 'string', 'description': \"A precise query that matches one very small category or product that needs to be searched for to find the products the user is looking for. If the user explicitly stated what they want, use that as a query. The query is as specific as possible to the product name or category mentioned by the user in its singular form, and don't contain any clarifiers like latest, newest, cheapest, budget, premium, expensive or similar. The query is always taken from the latest topic, if there is a new topic a new query is started. If the user speaks another language than English, translate their request into English (example: translate fia med knuff to ludo board game)!\"}, 'size': {'type': 'integer', 'description': 'number of products returned'}, 'min_price': {'type': 'integer', 'description': \"(Optional) Minimum price in local currency for the product searched for. Either explicitly stated by the user or implicitly inferred from a combination of the user's request and the kind of product searched for.\"}, 'max_price': {'type': 'integer', 'description': \"(Optional) Maximum price in local currency for the product searched for. Either explicitly stated by the user or implicitly inferred from a combination of the user's request and the kind of product searched for.\"}}, 'required': ['countryCode', 'q']}}}}]}), SimpleRequestChain(memory=None, callbacks=None, callback_manager=None, verbose=False, tags=None, metadata=None, request_method=<function get_openapi_chain.<locals>.<lambda> at 0x000001D6BE491D80>, output_key='response', input_key='function')], input_variables=['query'], output_variables=['response'], return_all=False)"
      ]
     },
     "execution_count": 4,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "chain = get_openapi_chain(\n",
    "    \"https://www.klarna.com/us/shopping/public/openai/v0/api-docs/\"\n",
    ")\n",
    "chain"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "{'products': [{'name': 'Cubavera Four Pocket Guayabera Shirt',\n",
       "   'url': 'https://www.klarna.com/us/shopping/pl/cl10001/3202055522/Clothing/Cubavera-Four-Pocket-Guayabera-Shirt/?utm_source=openai&ref-site=openai_plugin',\n",
       "   'price': '$24.99',\n",
       "   'attributes': ['Material:Polyester,Cotton',\n",
       "    'Target Group:Man',\n",
       "    'Color:Red,White,Blue,Black',\n",
       "    'Properties:Pockets',\n",
       "    'Pattern:Solid Color',\n",
       "    'Size:S,XL,L,M,XXL']},\n",
       "  {'name': 'Vineyard Vines Gingham On-The-Go brrr Classic Fit Shirt Crystal',\n",
       "   'url': 'https://www.klarna.com/us/shopping/pl/cl10001/3201938510/Clothing/Vineyard-Vines-Gingham-On-The-Go-brrr-Classic-Fit-Shirt-Crystal/?utm_source=openai&ref-site=openai_plugin',\n",
       "   'price': '$89.60',\n",
       "   'attributes': ['Material:Cotton',\n",
       "    'Target Group:Man',\n",
       "    'Color:Blue',\n",
       "    'Size:XL,XS,L,M']},\n",
       "  {'name': 'Wrangler Workwear 3W501 Twill Work Shirt-Forest Green-Reg-XL',\n",
       "   'url': 'https://www.klarna.com/us/shopping/pl/cl10001/3201808285/Clothing/Wrangler-Workwear-3W501-Twill-Work-Shirt-Forest-Green-Reg-XL/?utm_source=openai&ref-site=openai_plugin',\n",
       "   'price': '$25.99',\n",
       "   'attributes': ['Material:Cotton',\n",
       "    'Target Group:Man',\n",
       "    'Color:Blue,Beige,Green',\n",
       "    'Size:XL,L,M,XXL']},\n",
       "  {'name': 'Wrangler Mens Regular Fit Long Sleeve Button-Down Shirt',\n",
       "   'url': 'https://www.klarna.com/us/shopping/pl/cl10001/3201941362/Clothing/Wrangler-Mens-Regular-Fit-Long-Sleeve-Button-Down-Shirt/?utm_source=openai&ref-site=openai_plugin',\n",
       "   'price': '$25.99',\n",
       "   'attributes': ['Material:Cotton',\n",
       "    'Target Group:Man',\n",
       "    'Color:Blue',\n",
       "    'Size:L,M,XXL']},\n",
       "  {'name': \"Tommy Hilfiger Men's Check Classic-Fit Button-Down Shirt, Medium\",\n",
       "   'url': 'https://www.klarna.com/us/shopping/pl/cl10001/3201838510/Clothing/Tommy-Hilfiger-Men-s-Check-Classic-Fit-Button-Down-Shirt-Medium/?utm_source=openai&ref-site=openai_plugin',\n",
       "   'price': '$17.68',\n",
       "   'attributes': ['Material:Elastane/Lycra/Spandex,Cotton',\n",
       "    'Target Group:Man',\n",
       "    'Color:Gray,White,Blue',\n",
       "    'Size:S,XL,3XL,L,M,XXL',\n",
       "    'Sustainability Attributes :Organic']}]}"
      ]
     },
     "execution_count": 9,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "chain.run(\"What are some options for a men's large blue button down shirt\",callbacks=[Tracer])"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Test the singlife class"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "index_description:  IndexDescription(name='singlife', metric='cosine', replicas=1, dimension=1536.0, shards=1, pods=1, pod_type='p1', status={'ready': True, 'state': 'Ready'}, metadata_config=None, source_collection='')\n",
      "index_stats_response:  {'dimension': 1536,\n",
      " 'index_fullness': 0.1,\n",
      " 'namespaces': {'': {'vector_count': 3488}},\n",
      " 'total_vector_count': 3488}\n",
      "vectorstore created succesfully\n",
      "Model successfully loaded: cache=None verbose=False callbacks=None callback_manager=None tags=None metadata=None client=<class 'openai.api_resources.chat_completion.ChatCompletion'> model_name='gpt-3.5-turbo-0613' temperature=0.0 model_kwargs={} openai_api_key='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' openai_api_base='' openai_organization='' openai_proxy='' request_timeout=None max_retries=6 streaming=False n=1 max_tokens=None tiktoken_model_name=None\n"
     ]
    }
   ],
   "source": [
    "# all the init is done in the Singlife class\n",
    "singlife = Singlife()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "query = \"I am travelling to Japan for a ski trip with my family next week.What kind of travel insurance coverage do we need?\""
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Time to initialize overall chain: -0.00099945068359375\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "\u001b[34m\u001b[1mwandb\u001b[0m: Streaming LangChain activity to W&B at https://wandb.ai/shiok/Singlife/runs/pir6uf9q\n",
      "\u001b[34m\u001b[1mwandb\u001b[0m: `WandbTracer` is currently in beta.\n",
      "\u001b[34m\u001b[1mwandb\u001b[0m: Please report any issues to https://github.com/wandb/wandb/issues with the tag `langchain`.\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "\n",
      "\n",
      "\u001b[1m> Entering new RetrievalQA chain...\u001b[0m\n",
      "\n",
      "\u001b[1m> Finished chain.\u001b[0m\n",
      "\n",
      "\n",
      "\u001b[1m> Entering new LLMChain chain...\u001b[0m\n",
      "Prompt after formatting:\n",
      "\u001b[32;1m\u001b[1;3mGoal:Generate 15-30sec video script based on custom knowledge base (Information below) and user query. Two components 1.Scene assets descriptions 2.Subtitle script.\n",
      "\n",
      "Custom knowledge base:Based on the provided context of the Singlife Corporate Travel Insurance Policy, it appears to be more focused on business travel rather than personal travel. Therefore, it may not be the most suitable option for your family ski trip to Japan.\n",
      "\n",
      "For a family ski trip, you should consider a travel insurance policy specifically designed for leisure or personal travel. Look for a policy that includes coverage for trip cancellation or interruption, medical expenses, emergency medical evacuation, baggage loss or delay, and personal liability. Additionally, ensure that the policy covers winter sports activities such as skiing.\n",
      "\n",
      "It is important to carefully review the terms and conditions of the policy to ensure it provides adequate coverage for your specific needs during your ski trip in Japan.\n",
      "\n",
      "Using the above information, generate a video script that addresses this user query:\n",
      "\n",
      "\"I am travelling to Japan for a ski trip with my family next week.What kind of travel insurance coverage do we need?\".\n",
      "Return the generated video script in the style/format: Funny and sarcastic\u001b[0m\n",
      "\n",
      "\u001b[1m> Finished chain.\u001b[0m\n"
     ]
    },
    {
     "data": {
      "text/plain": [
       "{'list_of_scenes': [{'scene': 'Scene 1: Living Room',\n",
       "   'subtitles': ['Subtitle 1: [Family sitting on the couch]',\n",
       "    \"Subtitle 2: Dad: Hey everyone, guess what? We're going on a ski trip to Japan next week!\",\n",
       "    'Subtitle 3: Mom: That sounds amazing! But wait, do we need travel insurance?',\n",
       "    'Subtitle 4: Dad: Absolutely! Let me tell you what kind of coverage we need.']},\n",
       "  {'scene': 'Scene 2: Ski Resort',\n",
       "   'subtitles': ['Subtitle 1: [Family at the ski resort]',\n",
       "    \"Subtitle 2: Daughter: Look at all the snow! I can't wait to hit the slopes!\",\n",
       "    'Subtitle 3: Son: Yeah, but what if something goes wrong?',\n",
       "    \"Subtitle 4: Dad: Don't worry, we've got travel insurance to save the day!\"]},\n",
       "  {'scene': 'Scene 3: Insurance Office',\n",
       "   'subtitles': ['Subtitle 1: [Family at the insurance office]',\n",
       "    'Subtitle 2: Insurance Agent: Welcome! How can I assist you today?',\n",
       "    \"Subtitle 3: Dad: We're going on a ski trip to Japan and need travel insurance.\",\n",
       "    \"Subtitle 4: Insurance Agent: Great choice! For a ski trip, you'll need coverage for trip cancellation or interruption, medical expenses, emergency medical evacuation, baggage loss or delay, and personal liability.\",\n",
       "    'Subtitle 5: Mom: What about winter sports activities like skiing?',\n",
       "    'Subtitle 6: Insurance Agent: Absolutely! Make sure the policy covers winter sports activities too.',\n",
       "    'Subtitle 7: Dad: Perfect! We want to be protected on and off the slopes.',\n",
       "    \"Subtitle 8: Insurance Agent: I'll find the best policy for your family. Sit back and relax!\"]},\n",
       "  {'scene': 'Scene 4: Skiing on the Slopes',\n",
       "   'subtitles': ['Subtitle 1: [Family skiing on the slopes]',\n",
       "    'Subtitle 2: Daughter: Woo-hoo! This is so much fun!',\n",
       "    \"Subtitle 3: Son: And we're covered by travel insurance too!\",\n",
       "    \"Subtitle 4: Dad: That's right! No worries, just pure enjoyment!\"]},\n",
       "  {'scene': 'Scene 5: Closing',\n",
       "   'subtitles': ['Subtitle 1: [Family sitting by the fireplace]',\n",
       "    'Subtitle 2: Mom: Thank goodness we had travel insurance. It gave us peace of mind during our ski trip.',\n",
       "    'Subtitle 3: Dad: Absolutely! Always remember to choose the right coverage for your specific needs.',\n",
       "    \"Subtitle 4: Daughter: And don't forget to have fun!\",\n",
       "    'Subtitle 5: Son: Yeah, and make sure to do some cool tricks on the slopes!',\n",
       "    'Subtitle 6: Dad: Well said, kids! Stay safe and have a blast!']}]}"
      ]
     },
     "execution_count": 5,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "result = singlife.generateScript(query=query, model_name=\"gpt-3.5-turbo-0613\", video_style=\"Funny and sarcastic\")\n",
    "result"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "env",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.8.8"
  },
  "orig_nbformat": 4
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
